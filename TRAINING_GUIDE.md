# Руководство по системе обучения и уровней навыков

## Обзор системы

Система использует Q-learning для обучения выживших и маньяка. Агенты самообучаются через множество итераций (эпизодов), становясь лучше в выполнении своих задач.

## Система уровней навыков

### Уровни:

1. **Новичок (Novice)** - 0-500 эпизодов
   - Агенты только начинают учиться
   - Высокий epsilon (случайность действий)
   - Низкий процент успешных побегов

2. **Средний (Intermediate)** - 500-2000 эпизодов
   - Агенты начинают понимать базовые стратегии
   - Epsilon уменьшается
   - Средний процент успешных действий

3. **Продвинутый (Advanced)** - 2000-5000 эпизодов
   - Агенты хорошо знают карту и стратегии
   - Низкий epsilon (в основном оптимальные действия)
   - Высокий процент успешных действий

4. **Мастер (Master)** - 5000+ эпизодов
   - Агенты эксперты в своих задачах
   - Минимальный epsilon
   - Максимальный процент успешных действий

## Что БУДЕТ работать хорошо:

### ✅ Система Q-Learning:
- **Отлично работает** для дискретных состояний и действий
- Хорошо обучается простым стратегиям (идти к генератору, убегать от маньяка)
- Эффективно с небольшим пространством состояний

### ✅ Самообучение:
- Агенты **будут улучшаться** со временем
- Процент побегов будет расти с количеством эпизодов
- Q-table будет накапливать знания о хороших/плохих действиях

### ✅ Сохранение по уровням:
- Модели сохраняются автоматически каждые 100 эпизодов с суффиксом уровня
- Можно загрузить модель конкретного уровня для тестирования
- История статистики сохраняется

### ✅ Система наград:
- Поощрение за правильные действия работает хорошо
- Штрафы за неправильные действия помогают обучению
- Приоритеты (генераторы > выходы) хорошо усваиваются

## Что НЕ будет работать идеально:

### ❌ Проблемы Q-Learning в этой задаче:

1. **Критическое ограничение - Дискретизация состояний:**
   - Состояния представлены как кортежи (hunter_visible, gen_state, action_type, stuck)
   - **Проблема:** Это слишком грубая дискретизация
   - Реальные координаты не учитываются напрямую
   - Агенты могут "забыть" похожие ситуации

2. **Размер Q-table:**
   - При ограничении MAX_Q_TABLE_SIZE = 50000 некоторые состояния удаляются (LRU)
   - Может привести к "забыванию" редко встречающихся, но важных ситуаций

3. **Epsilon-greedy в тренировочной комнате:**
   - В `USE_TRAINING_ROOM = True` epsilon фиксирован на 0.05
   - Это ограничивает исследование новых стратегий после начального обучения

4. **Эвристика переопределяет обучение:**
   - `_heuristic_action()` в тренировочной комнате дает приоритет эвристике над Q-learning
   - Это мешает настоящему самообучению сложным стратегиям

### ⚠️ Ограничения системы:

1. **Локальные минимумы:**
   - Агенты могут застрять в субоптимальных стратегиях
   - Например, всегда идти в одно и то же место

2. **Переобучение на тренировочную комнату:**
   - Если обучать только в `USE_TRAINING_ROOM`, агенты не будут работать хорошо на случайных картах
   - Нужно чередовать обучение на разных картах

3. **Стабильность обучения:**
   - Процент побегов может колебаться, особенно на ранних этапах
   - Нужно много эпизодов (5000+) для стабильного результата

4. **Проблема с координатами:**
   - Состояния не включают точные координаты
   - Агенты могут путать похожие ситуации в разных частях карты

## Реализованные улучшения:

### ✅ 1. Расширенное пространство состояний:
```python
# Теперь используется: (hunter_visible, hunter_distance_state, hunter_direction, 
#                        gen_distance_state, gen_direction, action_type, stuck)
# Где:
# - distance_state: 0-4 (очень близко -> очень далеко)
# - direction: 0-7 (8 направлений по компасу)
```
**Улучшение:** Агенты теперь получают больше информации о расстояниях и направлениях, что позволяет лучше различать похожие ситуации.

### ✅ 2. Уменьшено влияние эвристики:
- Эвристика теперь используется адаптивно:
  - Первые 200 эпизодов: вероятность 80% -> 10% (уменьшается линейно)
  - После 200 эпизодов: вероятность 10% (минимальная)
- Позволяет Q-learning обучаться, сохраняя базовую помощь в начале

### ✅ 3. Adaptive Epsilon:
- Epsilon уменьшается быстрее при хорошей производительности
- Если процент побегов > 30%, используется дополнительный decay (0.998)
- Ускоряет обучение успешных агентов

### ✅ 4. Experience Replay:
- Реализован базовый Experience Replay
- Буфер на 1000 опытов
- Переобучение батчами по 32 опыта каждые 10 эпизодов
- Помогает стабилизировать обучение и лучше использовать прошлый опыт

## Дополнительные рекомендации (не реализовано):

### 2. Использовать Deep Q-Learning (DQN):
- Для более сложных состояний
- Автоматическое извлечение признаков
- Но требует больше ресурсов и библиотек (PyTorch/TensorFlow)

## Использование системы:

### Автоматическое сохранение:
- Модели автоматически сохраняются каждые 50 эпизодов (основной файл)
- Модели сохраняются каждые 100 эпизодов по уровню навыка:
  - `dbd_survivors_novice.pkl`
  - `dbd_survivors_intermediate.pkl`
  - `dbd_survivors_advanced.pkl`
  - `dbd_survivors_master.pkl`

### Загрузка моделей:
```python
# Загрузить модель конкретного уровня
await game.load_models_async(skill_level="novice")
await game.load_models_async(skill_level="master")

# Загрузить последнюю сохраненную модель
await game.load_models_async()
```

### Отслеживание прогресса:
- В логах отображается текущий уровень навыка
- Процент побегов за последние 100 эпизодов
- Средняя награда за эпизод

## Ожидаемые результаты:

### Новичок (0-500 эпизодов):
- Процент побегов: 0-10%
- Поведение: Случайное, часто застревает

### Средний (500-2000 эпизодов):
- Процент побегов: 10-40%
- Поведение: Базовые стратегии, иногда оптимизированные

### Продвинутый (2000-5000 эпизодов):
- Процент побегов: 40-70%
- Поведение: Хорошее понимание игры, эффективные маршруты

### Мастер (5000+ эпизодов):
- Процент побегов: 70-90%
- Поведение: Оптимизированные стратегии, минимум ошибок

## Заключение:

Q-learning **будет работать**, но с ограничениями из-за дискретизации состояний. Для более сложного обучения рекомендуется:
- Расширить пространство состояний
- Использовать более продвинутые методы (DQN, PPO)
- Уменьшить зависимость от эвристики
- Обучать на разнообразных картах

Текущая система подходит для демонстрации концепции и базового обучения, но для production-level ИИ нужны более продвинутые методы.

